{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JWH_assignment_DS_122_Sampling_Confidence_Intervals_and_Hypothesis_Testing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnwesleyharding/DS-Unit-1-Sprint-2-Statistics/blob/master/JWH_assignment_DS_122_Sampling_Confidence_Intervals_and_Hypothesis_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11OzdxWTM7UR",
        "colab_type": "text"
      },
      "source": [
        "## Assignment - Build a confidence interval\n",
        "\n",
        "A confidence interval refers to a neighborhood around some point estimate, the size of which is determined by the desired p-value. For instance, we might say that 52% of Americans prefer tacos to burritos, with a 95% confidence interval of +/- 5%.\n",
        "\n",
        "52% (0.52) is the point estimate, and +/- 5% (the interval $[0.47, 0.57]$) is the confidence interval. \"95% confidence\" means a p-value $\\leq 1 - 0.95 = 0.05$.\n",
        "\n",
        "In this case, the confidence interval includes $0.5$ - which is the natural null hypothesis (that half of Americans prefer tacos and half burritos, thus there is no clear favorite). So in this case, we could use the confidence interval to report that we've failed to reject the null hypothesis.\n",
        "\n",
        "But providing the full analysis with a confidence interval, including a graphical representation of it, can be a helpful and powerful way to tell your story. Done well, it is also more intuitive to a layperson than simply saying \"fail to reject the null hypothesis\" - it shows that in fact the data does *not* give a single clear result (the point estimate) but a whole range of possibilities.\n",
        "\n",
        "How is a confidence interval built, and how should it be interpreted? It does *not* mean that 95% of the data lies in that interval - instead, the frequentist interpretation is \"if we were to repeat this experiment 100 times, we would expect the average result to lie in this interval ~95 times.\"\n",
        "\n",
        "For a 95% confidence interval and a normal(-ish) distribution, you can simply remember that +/-2 standard deviations contains 95% of the probability mass, and so the 95% confidence interval based on a given sample is centered at the mean (point estimate) and has a range of +/- 2 (or technically 1.96) standard deviations.\n",
        "\n",
        "Different distributions/assumptions (90% confidence, 99% confidence) will require different math, but the overall process and interpretation (with a frequentist approach) will be the same.\n",
        "\n",
        "Your assignment - using the data from the prior module ([congressional voting records](https://archive.ics.uci.edu/ml/datasets/Congressional+Voting+Records)):\n",
        "\n",
        "\n",
        "### Confidence Intervals:\n",
        "1. Generate and numerically represent a confidence interval\n",
        "2. Graphically (with a plot) represent the confidence interval\n",
        "3. Interpret the confidence interval - what does it tell you about the data and its distribution?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ckcr4A4FM7cs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from scipy import stats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI3TRz1rlK9B",
        "colab_type": "code",
        "outputId": "d72d5a07-c000-4a79-f05d-c117c24670d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data\n",
        "\n",
        "df = pd.read_csv('house-votes-84.data', \n",
        "                 header=None,\n",
        "                 names=['party','handicapped-infants','water-project',\n",
        "                          'budget','physician-fee-freeze', 'el-salvador-aid',\n",
        "                          'religious-groups','anti-satellite-ban',\n",
        "                          'aid-to-contras','mx-missile','immigration',\n",
        "                          'synfuels', 'education', 'right-to-sue','crime','duty-free',\n",
        "                          'south-africa'])\n",
        "\n",
        "df = df.replace({'?': np.NaN, 'n': 0, 'y': 1})\n",
        "\n",
        "dem = df[df['party'] == \"democrat\"]\n",
        "rep = df[df['party'] == \"republican\"]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-10 05:09:32--  https://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/house-votes-84.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18171 (18K) [application/x-httpd-php]\n",
            "Saving to: ‘house-votes-84.data.5’\n",
            "\n",
            "\rhouse-votes-84.data   0%[                    ]       0  --.-KB/s               \rhouse-votes-84.data 100%[===================>]  17.75K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2019-10-10 05:09:33 (287 KB/s) - ‘house-votes-84.data.5’ saved [18171/18171]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCGYvcELzh0m",
        "colab_type": "text"
      },
      "source": [
        "standard error (of the mean) is the standard deviation divided by the sample size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HXho_vzNGYM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def confidence_interval(data, confidence=0.95):\n",
        "\n",
        "  mean = data.mean()\n",
        "  n = len(data)\n",
        "  sem = data.std()/n\n",
        "  margin = sem * stats.t.ppf((1 + confidence) / 2.0, n - 1)\n",
        "  return (mean, mean - margin, mean + margin)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O83N_ewGt26",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "23d326e2-0442-49c0-b340-ad08a6d56edd"
      },
      "source": [
        "(smean, cimin, cimax) = confidence_interval(df['south-africa'].dropna(), confidence = .95)\n",
        "print(f'Confidence Interval: {cimin} to {cimax} \\n')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confidence Interval: 0.8103665297210665 to 0.8150111137834652 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hG_pIdn1oWyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def showconfidence(series):\n",
        "\n",
        "  (smean, cimin, cimax) = confidence_interval(df[series].dropna(), confidence = .95)\n",
        "  \n",
        "  sns.distplot(df[series].dropna())\n",
        "  plt.axvline(x = cimin, color = 'o')\n",
        "  plt.axvline(x = cimax, color = 'o')\n",
        "  plt.axvline(x = smean, color = 'g')\n",
        "  plt.show()\n",
        "  \n",
        "  print(f'Confidence Interval: {cimin} to {cimax} \\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaJlgN7AoXVN",
        "colab_type": "code",
        "outputId": "72792c64-6c64-4989-e80a-e4ca39c65497",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "showconfidence(df['south-africa'])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e483cb47fa4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshowconfidence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'south-africa'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-e1840dd0ac5a>\u001b[0m in \u001b[0;36mshowconfidence\u001b[0;34m(series)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshowconfidence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;34m(\u001b[0m\u001b[0msmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcimin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcimax\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfidence_interval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.95\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2932\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2933\u001b[0m             indexer = self.loc._convert_to_indexer(key, axis=1,\n\u001b[0;32m-> 2934\u001b[0;31m                                                    raise_missing=True)\n\u001b[0m\u001b[1;32m   2935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[1;32m   1352\u001b[0m                 kwargs = {'raise_missing': True if is_setter else\n\u001b[1;32m   1353\u001b[0m                           raise_missing}\n\u001b[0;32m-> 1354\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         self._validate_read_indexer(keyarr, indexer,\n\u001b[1;32m   1160\u001b[0m                                     \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                                     raise_missing=raise_missing)\n\u001b[0m\u001b[1;32m   1162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1244\u001b[0m                 raise KeyError(\n\u001b[1;32m   1245\u001b[0m                     u\"None of [{key}] are in the [{axis}]\".format(\n\u001b[0;32m-> 1246\u001b[0;31m                         key=key, axis=self.obj._get_axis_name(axis)))\n\u001b[0m\u001b[1;32m   1247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of [Float64Index([1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0,\\n              ...\\n              1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0],\\n             dtype='float64', length=331)] are in the [columns]\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoQf17FQoXYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2YHSfn8LNFG",
        "colab_type": "text"
      },
      "source": [
        "### Chi-squared tests:\n",
        "4. Take a dataset that we have used in the past in class that has **categorical** variables. Pick two of those categorical variables and run a chi-squared tests on that data\n",
        "  - By hand using Numpy\n",
        "  - In a single line using Scipy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts_2yDJsMoHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://resources.lendingclub.com/LoanStats_2018Q4.csv.zip\n",
        "\n",
        "!unzip LoanStats_2018Q4.csv.zip\n",
        "\n",
        "df = pd.read_csv('LoanStats_2018Q4.csv', header = 1, skipfooter = 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTP-t3wMPKy8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7TPFvWpOskH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.describe(include = object)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDabmkIFMoKP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "observed = pd.crosstab(df['home_ownership'], df['installment']).values\n",
        "print(observed.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi-fPC2ioXbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chi_squared, p_value, dof, expected = stats.chi2_contingency(observed)\n",
        "\n",
        "print(f\"Chi-Squared: {chi_squared}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "print(f\"Degrees of Freedom: {dof}\") \n",
        "print(\"Expected: \\n\", np.array(expected))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qODVnfapjjr",
        "colab_type": "text"
      },
      "source": [
        "Questions\n",
        "\n",
        "What kind of features would the data have to possess to easily observe the difference between standard deviation and t-statistic.\n",
        "\n",
        "I'm lost on the significance of picking a null hypothesis, a confidence interval, and identifying an observation that really means something.  It seems like any way of considering the data is either obvious, insignificant, or irrelevant.  Is it all about who agrees with your perception of the null hypothesis?  Do you always want the smallest confidence interval you can achieve or one that's consistent across a domain?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ohsJhQUmEuS",
        "colab_type": "text"
      },
      "source": [
        "## Stretch goals:\n",
        "\n",
        "1. Write a summary of your findings, mixing prose and math/code/results. *Note* - yes, this is by definition a political topic. It is challenging but important to keep your writing voice *neutral* and stick to the facts of the data. Data science often involves considering controversial issues, so it's important to be sensitive about them (especially if you want to publish).\n",
        "2. Apply the techniques you learned today to your project data or other data of your choice, and write/discuss your findings here.\n",
        "3. Refactor your code so it is elegant, readable, and can be easily run for all issues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyJ3ySr7R2k9",
        "colab_type": "text"
      },
      "source": [
        "## Resources\n",
        "\n",
        "- [Interactive visualize the Chi-Squared test](https://homepage.divms.uiowa.edu/~mbognar/applets/chisq.html)\n",
        "- [Calculation of Chi-Squared test statistic](https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test)\n",
        "- [Visualization of a confidence interval generated by R code](https://commons.wikimedia.org/wiki/File:Confidence-interval.svg)\n",
        "- [Expected value of a squared standard normal](https://math.stackexchange.com/questions/264061/expected-value-calculation-for-squared-normal-distribution) (it's 1 - which is why the expected value of a Chi-Squared with $n$ degrees of freedom is $n$, as it's the sum of $n$ squared standard normals)"
      ]
    }
  ]
}